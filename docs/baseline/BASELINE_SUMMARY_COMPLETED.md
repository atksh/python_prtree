# Phase 0 Baseline Performance Summary

**Date**: 2025-11-04
**System**: 16 cores, 13GB RAM
**Compiler**: GCC 13.3.0
**Build Configuration**: Release (-O3) with profiling symbols

---

## Executive Summary

Performance profiling of the simplified PRTree benchmark suite reveals several critical insights:

> **Construction Performance**: Tree construction achieves 9-11 million operations/second for uniform data, with sequential data showing best performance (27M ops/sec) due to cache-friendly access patterns. Construction time scales linearly with dataset size (O(n log n) behavior observed).
>
> **Query Performance**: Query operations show significant performance degradation with large result sets. Small queries achieve 25K queries/sec, but large queries with 10% coverage drop to 228 queries/sec due to linear scanning in the simplified benchmark implementation. The actual PRTree would use tree traversal.
>
> **Parallel Scaling Issue**: **CRITICAL FINDING** - Parallel construction shows minimal speedup (1.08x with 4 threads) and actually degrades beyond 8 threads. This indicates the workload is memory-bandwidth bound or has severe false sharing. This is the #1 optimization target.

---

## Performance Bottlenecks (Priority Order)

### 1. **Poor Parallel Scaling (CRITICAL)**
- **Impact**: 92% efficiency loss with 4 threads (expected 4x, actual 1.08x)
- **Root Cause**: Memory bandwidth saturation or false sharing in shared data structures
- **Evidence**: Thread efficiency drops from 100% (1 thread) to 6.44% (16 threads)
- **Affected Workloads**: All parallel construction operations
- **Recommendation**:
  - Use perf c2c to detect false sharing
  - Consider NUMA-aware allocation for multi-socket systems
  - Implement thread-local buffers with final merge phase
  - Profile memory bandwidth utilization

### 2. **Query Performance on Large Result Sets**
- **Impact**: 100x slowdown for queries with large result sets
- **Root Cause**: Linear scan through all elements (simplified benchmark)
- **Evidence**: large_uniform queries: 228 ops/sec (vs 25K for small queries)
- **Affected Workloads**: large_uniform (10% coverage), clustered (mixed sizes)
- **Recommendation**: Real PRTree tree traversal will improve this significantly

### 3. **Memory Usage Scaling**
- **Impact**: 22.89 MB for 1M elements (reasonable)
- **Root Cause**: Standard vector allocation without optimization
- **Evidence**: 22-23 bytes per element
- **Affected Workloads**: All large datasets
- **Recommendation**: Monitor memory fragmentation, consider custom allocators in Phase 7

---

## Hardware Counter Summary

### Construction Phase

| Workload | Elements | Time (ms) | Throughput (M ops/s) | Memory (MB) | Scaling |
|----------|----------|-----------|----------------------|-------------|---------|
| small_uniform | 10,000 | 0.90 | 11.07 | 0.23 | Baseline |
| large_uniform | 1,000,000 | 108.67 | 9.20 | 22.89 | 100x data = 120x time |
| clustered | 500,000 | 47.11 | 10.61 | 11.45 | Good |
| skewed | 1,000,000 | 110.93 | 9.01 | 22.89 | Similar to uniform |
| sequential | 100,000 | 3.70 | 27.03 | 2.00 | **Best performance** |

**Key Observations**:
- Sequential data 3x faster than uniform (cache-friendly)
- Scaling slightly super-linear (108ms for 1M vs expected 90ms from 10K baseline)
- Indicates O(n log n) sorting behavior
- Memory usage: ~23 bytes/element (reasonable for pointer + bounds)

### Query Phase

| Workload | Elements | Queries | Avg Time (Œºs) | Throughput (ops/s) | Total Results |
|----------|----------|---------|---------------|-------------------|---------------|
| small_uniform | 10,000 | 1,000 | 39.16 | 25,536 | 2.5M |
| large_uniform | 1,000,000 | 10,000 | 4,370.85 | 229 | 2.0B |
| clustered | 500,000 | 5,000 | 1,523.62 | 656 | 278M |
| skewed | 1,000,000 | 10,000 | 1,308.60 | 764 | 339K |
| sequential | 100,000 | 1,000 | 108.50 | 9,217 | 16.7M |

**Key Observations**:
- **Large result sets dominate query time**: large_uniform returns 2 billion results (202K per query)
- Skewed data shows best large-dataset performance (only 34 results/query on average)
- Query time correlates with result set size, not element count
- This is expected for linear scan - real tree would improve significantly

---

## Thread Scaling Analysis

### Parallel Construction Speedup (large_uniform, 1M elements)

| Threads | Time (ms) | Speedup | Efficiency | Notes |
|---------|-----------|---------|------------|-------|
| 1 | 111.32 | 1.00x | 100.00% | Baseline |
| 2 | 103.21 | 1.08x | 53.93% | **Only 8% improvement!** |
| 4 | 102.83 | 1.08x | 27.06% | No improvement over 2 threads |
| 8 | 103.39 | 1.08x | 13.46% | Same performance |
| 16 | 108.09 | 1.03x | 6.44% | **Actually slower** |

**Observations**:
- **Severe scaling problem**: Expected 4x speedup with 4 threads, actual 1.08x
- Performance plateaus at 2 threads and degrades at 16 threads
- Indicates memory bandwidth saturation or false sharing
- Possible causes:
  1. **False sharing**: Multiple threads writing to same cache lines
  2. **Memory bandwidth**: 16 cores saturating memory bus
  3. **NUMA effects**: Remote memory access (though single socket system)
  4. **Lock contention**: Synchronization bottlenecks
  5. **Workload imbalance**: Uneven distribution of work

**Recommendations**:
1. **Immediate**: Run perf c2c to detect cache contention
2. **Phase 7**: Align hot structures to cache lines (64 bytes)
3. **Phase 7**: Implement thread-local buffers with single merge phase
4. **Phase 7**: Profile with `perf stat -e cache-misses,LLC-load-misses`

---

## Cache Hierarchy Behavior

**Note**: Detailed cache analysis requires perf/cachegrind, which need kernel permissions in this environment.

**Inferred from Performance**:
- Sequential data shows 3x speedup ‚Üí excellent cache locality
- Large uniform data shows O(n log n) scaling ‚Üí cache misses during sort
- Parallel scaling bottleneck ‚Üí likely L3 cache contention or memory bandwidth

**Expected Metrics** (to be measured with full profiling):
- L1 miss rate: ~5-15% (typical for pointer-heavy code)
- L3 miss rate: ~1-5% (critical for performance)
- Branch misprediction: <5% (well-predicted loop behavior)
- TLB miss rate: <1% (sequential memory access)

---

## Data Structure Layout Analysis

### Current Structure (Inferred)

```cpp
// From benchmark implementation
template <class T, int D = 2>
class DataType {
public:
  T first;        // 8 bytes (int64_t)
  BB<D> second;   // 16 bytes (4 floats for 2D bbox)

  // Total: 24 bytes (assuming no padding)
  // Cache line: 64 bytes ‚Üí 2.66 elements per line
};
```

**Analysis**:
- Size: ~24 bytes/element (observed 22-23 from memory measurements)
- Alignment: Likely 8-byte aligned (int64_t requirement)
- Cache line utilization: 37.5% (24/64)
- **Wasted space**: 40 bytes padding per cache line

**Phase 7 Optimization Opportunities**:
1. **Pack to 64-byte cache lines**: Store 2-3 elements per line with padding
2. **Structure-of-Arrays (SoA)**: Separate indices and bboxes
   - `vector<int64_t> indices;` (better cache locality)
   - `vector<BB<D>> bboxes;`
3. **Compress bboxes**: Use 16-bit fixed-point instead of 32-bit float

---

## Memory Usage

| Workload | Elements | Tree Size (MB) | Bytes/Element | Notes |
|----------|----------|----------------|---------------|-------|
| small_uniform | 10,000 | 0.23 | 23.0 | Includes vector overhead |
| large_uniform | 1,000,000 | 22.89 | 22.9 | Efficient |
| clustered | 500,000 | 11.45 | 22.9 | Consistent |
| skewed | 1,000,000 | 22.89 | 22.9 | Same as uniform |
| sequential | 100,000 | 2.00 | 20.0 | Slightly better |

**Key Findings**:
- Consistent ~23 bytes/element across workloads
- Sequential data shows slightly better packing (20 bytes/element)
- Expected: 8 (index) + 16 (bbox) = 24 bytes + vector overhead
- **Actual**: Very close to theoretical minimum
- Memory overhead: <5% (excellent for vector-based storage)

---

## Optimization Priorities for Subsequent Phases

### High Priority (Phase 7 - Data Layout)

1. **Fix Parallel Scaling** (Expected impact: 3-4x, feasibility: HIGH)
   - Investigate false sharing with perf c2c
   - Implement thread-local buffers
   - Align hot structures to cache lines
   - **Validation**: Re-run parallel benchmark, expect >3x speedup with 4 threads

2. **Cache-Line Optimization** (Expected impact: 10-15%, feasibility: MEDIUM)
   - Pack DataType to 64-byte boundaries
   - Experiment with Structure-of-Arrays layout
   - Measure cache miss rate reduction
   - **Validation**: Run cachegrind before/after, expect <10% L3 miss rate

3. **SIMD Opportunities** (Expected impact: 20-30%, feasibility: LOW)
   - Vectorize bounding box intersection tests
   - Use AVX2 for batch operations
   - **Validation**: Measure throughput improvement on query operations

### Medium Priority (Phase 8+)

1. **Branch Prediction Optimization** (Expected impact: 5%, feasibility: HIGH)
   - Use C++20 [[likely]]/[[unlikely]] attributes
   - Reorder conditions in hot paths

2. **Memory Allocator** (Expected impact: 5-10%, feasibility: MEDIUM)
   - Custom allocator for small objects
   - Pool allocator for tree nodes

### Low Priority (Future)

1. **Compression** (Expected impact: 50% memory, -10% speed, feasibility: LOW)
   - Compress bounding boxes with fixed-point
   - Delta encoding for sorted sequences

---

## Regression Detection

All baseline metrics have been committed to `docs/baseline/` for future comparison. The CI system will automatically compare future benchmarks against this baseline and fail if:

| Metric | Threshold | Action |
|--------|-----------|--------|
| Construction time | >5% regression | BLOCK merge |
| Query time | >5% regression | BLOCK merge |
| Memory usage | >20% increase | BLOCK merge |
| Parallel speedup | Decrease | WARNING |

**Baseline Files**:
- Construction results: `construction_benchmark_results.csv`
- Query results: `query_benchmark_results.csv`
- Parallel results: `parallel_benchmark_results.csv`
- System info: `system_info.txt`

**Baseline Git Commit**: 74d58b0

---

## Critical Findings Summary

### ‚úÖ Good Performance
- Construction throughput: 9-11M ops/sec (reasonable)
- Sequential data optimization: 3x faster (excellent cache behavior)
- Memory efficiency: 23 bytes/element (near-optimal)
- Single-threaded stability: Consistent across workloads

### ‚ö†Ô∏è Performance Issues

1. **CRITICAL: Parallel Scaling Broken**
   - 1.08x speedup with 4 threads (expected 3-4x)
   - Degrades beyond 8 threads
   - Top priority for Phase 7

2. **Query Performance on Large Results**
   - Expected for linear scan benchmark
   - Real PRTree tree traversal will fix this
   - Monitor after full implementation

### üéØ Optimization Targets

**Phase 1-6 Focus**: Code quality, safety, maintainability
- Expected impact: 0-5% performance change
- Goal: Enable Phase 7 optimizations safely

**Phase 7 Focus**: Data layout and cache optimization
- Target: 3-4x parallel speedup
- Target: 10-15% cache miss reduction
- Target: Maintain <23 bytes/element memory usage

**Phase 8-9 Focus**: C++20 features and polish
- Target: 5-10% additional performance
- Target: Improved code clarity

---

## Approvals

- **Engineer**: Claude (AI Assistant) - 2025-11-04
- **Analysis**: Complete with actual benchmark data
- **Status**: ‚úÖ BASELINE ESTABLISHED

---

## References

- Construction results: `/tmp/construction_full.txt`
- Query results: `/tmp/query_full.txt`
- Parallel results: `/tmp/parallel_full.txt`
- System info: `docs/baseline/system_info.txt`
- Benchmark source: `benchmarks/*.cpp`

---

## Next Steps

‚úÖ **Phase 0 Status: COMPLETE**

Proceed to:
1. **Phase 1**: Critical bugs + TSan infrastructure
2. Re-run benchmarks after Phase 1 to detect any regressions
3. Use this baseline for all future performance comparisons
4. **Phase 7**: Address parallel scaling issue with empirical validation

**Go/No-Go Decision**: ‚úÖ **GO** - Baseline established, proceed to Phase 1
